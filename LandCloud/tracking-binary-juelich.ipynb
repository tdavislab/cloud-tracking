{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b96a65",
   "metadata": {},
   "source": [
    "# pFGW Framework for Feature Matching & Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8a2a508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import networkx as nx\n",
    "import gc\n",
    "import time\n",
    "\n",
    "sys.path.append(\"../GWMT/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa6fc859-0f0c-4dbb-a784-90b62f7baf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GWMT import *\n",
    "import readMergeTree as rmt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8460f2",
   "metadata": {},
   "source": [
    "## Data Input "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d510d81b",
   "metadata": {},
   "source": [
    "This is an example to read our preset datasets. Our merge tree data is split into two files: ``treeEdges_monoMesh_*.txt`` and ``treeNodes_monoMesh_*.txt``, representing the data for tree edges and tree nodes, respectively.\n",
    "\n",
    "To read your own dataset, your tree data should be saved in a ``nx.Graph`` object, in which each node has properties for its spacial coordinates (e.g., \"x\", \"y\"), its scalar value (e.g., \"height\"), and its critical type (0: minimum, 1: saddle, 2: maximum. This setting cannot be changed). \n",
    "Besides, you also need to provide the node id for the root node of the tree.\n",
    "\n",
    "The tree data should be stored in a ``GWMergeTree`` object.\n",
    "\n",
    "==========================================================\n",
    "\n",
    "Our tree edge data format:\n",
    "\n",
    "a_0, b_0  \n",
    "a_1, b_1  \n",
    "...  \n",
    "a_{|E|-1}, b_{|E|-1}\n",
    "\n",
    "Each row describes the indices of two nodes that the edge connecting in between. Edges are undirected.\n",
    "\n",
    "==========================================================\n",
    "\n",
    "Our tree node data format:\n",
    "\n",
    "x_0, y_0, z_0, scalar_0, type_0  \n",
    "x_1, y_1, z_1, scalar_1, type_1  \n",
    "...  \n",
    "x_{|V|-1}, y_{|V|-1}, z_{|V|-1}, scalar_{|V|-1}, type_{|V|-1}\n",
    "\n",
    "Each row has five components: the \"x\", \"y\", \"z\" coordinates, the scalar value, and the critical point type for the node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e54b0f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 20180501_juelich\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Working on 20180501_juelich\n",
      "Initially removing 6350 leaves.\n",
      "Initially removing 6415 leaves.\n",
      "Benchmark information - Simplify Merge Tree: 15346 4032 2 1.8365094000000006\n",
      "Benchmark information - Simplify Merge Tree: 30667 7799\n",
      "Working on 20180501_juelich\n",
      "Working on 20180501_juelich\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23167"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset choices: [\"HeatedCylinder\", \"IonizationFront\", \"UnsteadyCylinderFlow\", \"VortexStreet\", \"CloudDensity\"]\n",
    "maxima_only = True\n",
    "# value_thres = 2.0\n",
    "\n",
    "thres_dict_by_time = {\n",
    "    \"morning\": 9,\n",
    "    \"afternoon\": 10,\n",
    "    \"late-afternoon\": 9\n",
    "}\n",
    "\n",
    "time_period = {\n",
    "    \"morning\": [600, 900],  # 0, 36\n",
    "    \"afternoon\": [901, 1500],  # 37, 108\n",
    "    \"late-afternoon\": [1501, 1800],  # 109, \n",
    "}\n",
    "\n",
    "def get_time_str(hrtime):\n",
    "    int_hrtime = int(hrtime)\n",
    "    for key in time_period:\n",
    "        if (int_hrtime >= time_period[key][0]) and (int_hrtime <= time_period[key][1]):\n",
    "            return key\n",
    "\n",
    "def get_hrtime_by_filename(filename):\n",
    "    fn1 = filename.replace(\".txt\", \"\").replace(\".npy\", \"\")\n",
    "    datetime = fn1.split(\"_\")[-1]\n",
    "    date, hrtime = datetime.split(\"t\")\n",
    "    return hrtime\n",
    "\n",
    "gwmt_list = dict()\n",
    "\n",
    "mt_list = dict()\n",
    "root_list = dict()\n",
    "\n",
    "time_list = dict()\n",
    "period_list = dict()\n",
    "\n",
    "datasets = [\"20180501_juelich\"] # , \"20180623_juelich\", \"20190512_juelich\"]\n",
    "for dataset in datasets:\n",
    "    print(\"Working on\", dataset)\n",
    "    mt_list[dataset] = []\n",
    "    root_list[dataset] = []\n",
    "    \n",
    "    time_list[dataset] = []\n",
    "    period_list[dataset] = []\n",
    "    \n",
    "    dataset_path = os.path.join(\"data\", dataset)\n",
    "    for froot, di, files in os.walk(dataset_path):\n",
    "\n",
    "        def key(s):\n",
    "            try:\n",
    "                int(s)\n",
    "                return int(s)\n",
    "            except ValueError:\n",
    "                return len(files) + 1\n",
    "\n",
    "        def isSegmentation(s: str):\n",
    "            return \"segmentation\" in s\n",
    "\n",
    "        def endsWithTxt(s: str):\n",
    "            return s.endswith(\"txt\")\n",
    "\n",
    "        def endsWithNpy(s: str):\n",
    "            return s.endswith(\"npy\")\n",
    "\n",
    "        txt_files = list(filter(endsWithTxt, files))\n",
    "        txt_files.sort(key=lambda x: key(x.split(\".\")[0].split(\"_\")[-1]))\n",
    "\n",
    "        # You need to specify the root node type. Choices: [\"minimum\", \"maximum\"]\n",
    "        # (Avoid specifying merge tree type to avoid confusion between split tree and join tree in different contexts)\n",
    "        for file in txt_files:\n",
    "            hrtime = get_hrtime_by_filename(file)\n",
    "            value_thres = thres_dict_by_time[get_time_str(hrtime)]\n",
    "            trees, roots = rmt.get_trees(os.path.join(dataset_path, file), root_type=\"minimum\", threshold=value_thres)\n",
    "            if len(trees) > 0:\n",
    "                time_list[dataset].append(hrtime)\n",
    "                period_list[dataset].append(get_time_str(hrtime))\n",
    "            mt_list[dataset].extend(trees)\n",
    "            root_list[dataset].extend(roots)\n",
    "\n",
    "    assert (len(root_list[dataset]) == len(mt_list[dataset]))\n",
    "    assert (len(time_list[dataset]) == len(mt_list[dataset]))\n",
    "    assert (len(period_list[dataset]) == len(mt_list[dataset]))\n",
    "    \n",
    "# ================================================================== #\n",
    "    \n",
    "# Let's not oversimplify the merge tree, because now we need many nodes as anchor points\n",
    "\n",
    "# This serves for removing the very-small cloud system from the results entirely\n",
    "# This should be very small\n",
    "disappear_volume_threshold = 1\n",
    "\n",
    "# This is to reduce the number of anchor points (but not remove) for cloud systems\n",
    "# This can be a bit large\n",
    "\n",
    "def get_volume_thres(time_str):\n",
    "    volume_thres_by_time_dict = {\n",
    "        \"morning\": 5,\n",
    "        \"afternoon\": 5,\n",
    "        \"late-afternoon\": 5,\n",
    "    }\n",
    "    return volume_thres_by_time_dict[time_str]\n",
    "\n",
    "simplified_mt_list = dict()\n",
    "simplified_root_list = dict()\n",
    "\n",
    "max_raw_nodes = 0\n",
    "max_simplified_nodes = 0\n",
    "total_raw_nodes = 0\n",
    "total_simplified_nodes = 0\n",
    "n_runs = 0\n",
    "mt_simplification_runtime = 0\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Working on\", dataset)\n",
    "    simplified_mt_list[dataset] = [None for i in range(len(mt_list[dataset]))]\n",
    "    simplified_root_list[dataset] = [None for i in range(len(mt_list[dataset]))]\n",
    "\n",
    "    idx = 0\n",
    "\n",
    "    for i in range(len(mt_list[dataset])):\n",
    "        mt = mt_list[dataset][i]\n",
    "        \n",
    "        tstr = period_list[dataset][i]\n",
    "        tt = time_list[dataset][i]\n",
    "        value_thres = thres_dict_by_time[get_time_str(tt)]\n",
    "        if True:\n",
    "            # For benchmark purpose\n",
    "            simplify_t0 = time.perf_counter()\n",
    "            \n",
    "            _, simp_mt = volume_simplify_mt(mt, \n",
    "                                            vol_thres=get_volume_thres(tstr), \n",
    "                                            disappear_vol_thres=disappear_volume_threshold, \n",
    "                                            vol_name=\"volume\", \n",
    "                                            stop_saddle_val=value_thres)\n",
    "            # For benchmark purpose\n",
    "            simplify_dt = time.perf_counter() - simplify_t0\n",
    "            n_runs += 1\n",
    "            mt_simplification_runtime += simplify_dt\n",
    "            \n",
    "            simplified_mt_list[dataset][idx] = simp_mt\n",
    "            simplified_root_list[dataset][idx] = simp_mt.root\n",
    "            \n",
    "            max_raw_nodes = max(max_raw_nodes, mt.number_of_nodes())\n",
    "            max_simplified_nodes = max(max_simplified_nodes, simp_mt.number_of_nodes())\n",
    "            total_raw_nodes += mt.number_of_nodes()\n",
    "            total_simplified_nodes += simp_mt.number_of_nodes()\n",
    "        else:\n",
    "            simplified_mt_list[dataset][idx] = mt\n",
    "            simplified_root_list[dataset][idx] = mt.root\n",
    "\n",
    "        nn_smaller_than_thres = 0\n",
    "        for node in simplified_mt_list[dataset][idx].nodes():\n",
    "            if simplified_mt_list[dataset][idx].nodes[node][\"height\"] < value_thres:\n",
    "                if simplified_mt_list[dataset][idx].nodes[node][\"type\"] == 2:\n",
    "                    nn_smaller_than_thres += 1\n",
    "#         print(tt, mt.number_of_nodes(), simplified_mt_list[dataset][idx].number_of_nodes(), nn_smaller_than_thres)\n",
    "        idx += 1\n",
    "\n",
    "print(\"Benchmark information - Simplify Merge Tree:\", max_raw_nodes, max_simplified_nodes, n_runs, mt_simplification_runtime)\n",
    "print(\"Benchmark information - Simplify Merge Tree:\", total_raw_nodes, total_simplified_nodes)\n",
    "        \n",
    "for dataset in datasets:\n",
    "    lmt = len(mt_list[dataset])\n",
    "    mt_list[dataset] = [None] * lmt\n",
    "    \n",
    "gwmt_list = dict()\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Working on\", dataset)\n",
    "    gwmt_list[dataset] = [GWMergeTree(simplified_mt_list[dataset][i], simplified_root_list[dataset][i]) for i in range(len(mt_list[dataset]))]\n",
    "    print(len(gwmt_list[dataset]))\n",
    "\n",
    "for dataset in datasets:\n",
    "    lmt = len(simplified_mt_list[dataset])\n",
    "    simplified_mt_list[dataset] = [None] * lmt\n",
    "    simplified_root_list[dataset] = [None] * lmt\n",
    "\n",
    "del mt_list\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ccc809",
   "metadata": {},
   "source": [
    "## Parameter Initialization\n",
    "\n",
    "We now specify the parameters to be passed to the pFGW framework, including the following:\n",
    "\n",
    "*spacial_scale*: a float number denoting the spacial length of the scalar field. (e.g., $max(height, width)$ for a 2D rectangular scalar field).  \n",
    "         This is needed for normalizing the spacial coordinates for nodes.  \n",
    "         This number can be flexible. However, it affects the optimal $\\alpha$ value. Please make sure that the normalization is reasonable. A recommended range for the maximum Euclidean distance between any pair of critical points after normalization is $[0.5, 2.0]$.\n",
    "         \n",
    "\n",
    "*labels*: a list of strings denoting the spacial coordinate labels. (e.g., [\"x\", \"y\"] for a 2D scalar field)\n",
    "\n",
    "*scalar_name*: the name of the scalar field in GWMergeTree objects.\n",
    "\n",
    "*edge_weight_name*: the name of the weight of edges in GWMergeTree objects.\n",
    "\n",
    "*weight_mode*: the strategy to encode $W$. Choices: [\"shortestpath\", \"lca\"].\n",
    "\n",
    "*prob_distribution*: the strategy to encode $p$. Choices: [\"uniform\", \"ancestor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d9819ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"CloudDensity\" in dataset:\n",
    "    spacial_scale = 255 \n",
    "elif \"CPPin202308\" in dataset:\n",
    "    spacial_scale = 932\n",
    "elif \"juelich\" in dataset:\n",
    "    spacial_scale = 427\n",
    "    \n",
    "max_value = 150\n",
    "labels = [\"x\", \"y\", \"z\"]\n",
    "scalar_name = \"height\"\n",
    "edge_weight_name = \"weight\"\n",
    "weight_mode = \"shortestpath\"\n",
    "prob_distribution = \"uniform\"\n",
    "fully_initialized = False\n",
    "\n",
    "# lca-threshold\n",
    "intrinsic_rescale = 1\n",
    "if weight_mode == \"lca\":\n",
    "    intrinsic_rescale = max_value\n",
    "elif weight_mode == \"shortestpath\":\n",
    "    intrinsic_rescale = max_value * 2\n",
    "    \n",
    "all_scales = [spacial_scale, spacial_scale, spacial_scale, intrinsic_rescale]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "305bea04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 20180501_juelich\n"
     ]
    }
   ],
   "source": [
    "# Validify the GWMergeTree object. This is not mandatory, but recommended to check whether your data input format is correct\n",
    "for dataset in datasets:\n",
    "    print(\"Working on\", dataset)\n",
    "    for tree in gwmt_list[dataset]:\n",
    "        tree.label_validation(labels, scalar_name, edge_weight_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37304c4",
   "metadata": {},
   "source": [
    "## Framework Initialization\n",
    "\n",
    "Initializing the pFGW framework with given parameters and data input. There is an output for the normalization process consisting $L+1$ elements, where the first $L$ elements are $\\frac{1}{rescaling\\_factor}$ for coordinate labels ($L$ coordinate labels in total), and the last element is $\\frac{1}{rescaling\\_factor}$ for scalar function values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80fe6301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 20180501_juelich\n",
      "WARNING! We do not compute the scale for all time steps.\n",
      "Please provide the scale for normalization when intialize steps.\n"
     ]
    }
   ],
   "source": [
    "pfgws = dict()\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Working on\", dataset)\n",
    "    pfgws[dataset] = GWTracking(\n",
    "        gwmt_list[dataset],\n",
    "        spacial_scale,\n",
    "        labels,\n",
    "        scalar_name=scalar_name,\n",
    "        edge_weight_name=edge_weight_name,\n",
    "        weight_mode=weight_mode,\n",
    "        prob_distribution=prob_distribution,\n",
    "        tracking_maxima_only=maxima_only,\n",
    "        maxima_only=maxima_only,\n",
    "        fully_initialized=fully_initialized,\n",
    "        scalar_threshold=value_thres\n",
    "    )\n",
    "    \n",
    "    pfgws[dataset].set_all_scales(all_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250f8177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e47c09a",
   "metadata": {},
   "source": [
    "## Parameter Tuning\n",
    "\n",
    "We present a demo for our parameter tuning process in this section.\n",
    "\n",
    "This is not the only way for parameter tuning. We aim to optimize the one-to-one matching result in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16a21b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# The optional parameter \"amijo\" determines the way for OT solver to converge, which may change the result\n",
    "# Either picking False or True follows our framework pipeline. They just may reach different local optimal solutions.\n",
    "\n",
    "amijo = False\n",
    "max_dist_tuning = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab6275e",
   "metadata": {},
   "source": [
    "### Actual tracking with tuning the m parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dac6f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_runs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfaa4d26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 20180501_juelich\n",
      "alpha = 0.2\n",
      "CList shape: (2017,)\n",
      "Finished intializing timestep 0\n",
      "CList shape: (1884,)\n",
      "Finished intializing timestep 1\n",
      "Init Runtime #0 106.5332406\n",
      "Init Runtime #1 94.2633916\n",
      "(ms, dists): [(0.805, 0.015710079467211653)]\n",
      "Cannot find the possible m with the current max_dist!\n",
      "pFGW runtime #0 380.1335666\n",
      "The optimal m for timestep 0 vs. 1 is 0.805.\n",
      "Finished releasing timetep 0\n",
      "INIT runtime: [106.5332406, 94.2633916]\n",
      "pFGW runtime: [380.1335666]\n",
      "Initialization average runtime: 100.3983161\n",
      "pFGW average runtime: 380.1335666\n"
     ]
    }
   ],
   "source": [
    "# New!!! We are using auto parameter tuning strategies\n",
    "# For cloud tracking project, there is a forced search range for other cloud tracking tools\n",
    "# Therefore, we can safely assume the max matched distance without tuning\n",
    "max_dist = 0.014\n",
    "alpha_list = [0.2]\n",
    "m_range = [0.60, 0.90]\n",
    "\n",
    "max_workers = 7\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Working on\", dataset)\n",
    "    timesteps = list(range(len(pfgws[dataset].trees)))[:109]\n",
    "\n",
    "    dataset_str = dataset\n",
    "    parameter_tuning_path = os.path.join(\"binary-parameter-tuning\", \"{}\".format(dataset_str))\n",
    "    os.makedirs(parameter_tuning_path, exist_ok=True)\n",
    "    oc_path = os.path.join(parameter_tuning_path, \"ocs\")\n",
    "    os.makedirs(oc_path, exist_ok=True)\n",
    "\n",
    "    for alpha in alpha_list:\n",
    "        if (dataset in finished_runs) and (alpha in finished_runs[dataset]):\n",
    "            continue\n",
    "        print(\"alpha =\", alpha)\n",
    "        best_ms, best_ocs, m_searchspace, dist_values, init_rt, pfgw_rt = pfgws[dataset].adaptive_m_tuning_binary_search(timesteps,\n",
    "                                                                                             alpha, \n",
    "                                                                                             max_dist, \n",
    "                                                                                             amijo, \n",
    "                                                                                             m_range, \n",
    "                                                                                             max_workers, \n",
    "                                                                                             metric=\"l2\", \n",
    "                                                                                             prob_rubric=\"nonzero\",\n",
    "                                                                                             benchmark=True)\n",
    "        \n",
    "        print(\"Initialization average runtime:\", init_rt)\n",
    "        print(\"pFGW average runtime:\", pfgw_rt)\n",
    "        save_binary_parameter_tuning(parameter_tuning_path, oc_path, alpha, best_ms, best_ocs, m_searchspace, dist_values)\n",
    "        \n",
    "        if dataset not in finished_runs:\n",
    "            finished_runs[dataset] = [alpha]\n",
    "        else:\n",
    "            finished_runs[dataset].append(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c528829a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a09175a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 20180501_juelich\n",
      "alpha = 0.2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(alpha)))\n\u001b[1;32m---> 18\u001b[0m best_ms, best_ocs, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mload_binary_parameter_tuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameter_tuning_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moc_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_root, \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mround\u001b[39m(alpha, \u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m     21\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\desktop\\ResearchProjects\\GWMT-Cloud-Revision\\utilities.py:1243\u001b[0m, in \u001b[0;36mload_binary_parameter_tuning\u001b[1;34m(parameter_tuning_path, oc_path, alpha)\u001b[0m\n\u001b[0;32m   1240\u001b[0m alpha_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mround\u001b[39m(alpha, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   1241\u001b[0m best_ms \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(pjoin(parameter_tuning_path, \n\u001b[0;32m   1242\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_ms_alpha_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(alpha_str)))\n\u001b[1;32m-> 1243\u001b[0m N_ocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbest_ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1244\u001b[0m best_ocs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_ocs):\n",
      "\u001b[1;31mTypeError\u001b[0m: len() of unsized object"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(\"Working on\", dataset)\n",
    "    timesteps = list(range(len(pfgws[dataset].trees)))[:109]\n",
    "\n",
    "    # Step 4. We save the best-tuned results into the results folder\n",
    "    dataset_str = dataset\n",
    "    parameter_tuning_path = os.path.join(\"binary-parameter-tuning\", \"{}\".format(dataset_str))\n",
    "\n",
    "    oc_path = os.path.join(parameter_tuning_path, \"ocs\")\n",
    "\n",
    "    output_root = os.path.join(\"./initial-output/\", dataset_str)\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "    for e, alpha in enumerate(alpha_list):\n",
    "        if alpha not in finished_runs[dataset]:\n",
    "            continue\n",
    "            \n",
    "        print(\"alpha = {}\".format(str(alpha)))\n",
    "        best_ms, best_ocs, _, _ = load_binary_parameter_tuning(parameter_tuning_path, oc_path, alpha)\n",
    "\n",
    "        output_path = os.path.join(output_root, str(round(alpha, 1)))\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        for em, m in enumerate(best_ms):\n",
    "            print(str('%.3f' % m))\n",
    "            # We filter out coupling information unrelated to maxima\n",
    "            # Note: the filtered coupling matrix does not sum to m\n",
    "            id1 = timesteps[em]\n",
    "            id2 = timesteps[em+1]\n",
    "\n",
    "            oc = best_ocs[em]\n",
    "            oc_maxima = oc # pfgw.filtered_oc(id1, id2, oc)\n",
    "            np.savetxt(os.path.join(output_path, \"oc_{}_{}.txt\".format(str(id1), str(id2))), oc_maxima)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9629d57c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d897b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
