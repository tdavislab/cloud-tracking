{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2583b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GWMT import *\n",
    "import readMergeTree as rmt\n",
    "import os\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7fb82a",
   "metadata": {},
   "source": [
    "# Load data\n",
    "* The simplified merge tree\n",
    "* The scalar field data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ae8f62",
   "metadata": {},
   "source": [
    "## loading the merge tree and the scalar field data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66caa85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"CPPin20230808_0percent\"]\n",
    "# datasets = [\"20180501_juelich\", \"20180623_juelich\", \"20190512_juelich\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "381acb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n"
     ]
    }
   ],
   "source": [
    "maxima_only = True\n",
    "# value_thres = 2.0\n",
    "\n",
    "# thres_dict_by_time = {\n",
    "#     \"morning\": 9,\n",
    "#     \"afternoon\": 10,\n",
    "#     \"late-afternoon\": 9\n",
    "# }\n",
    "\n",
    "# time_period = {\n",
    "#     \"morning\": [600, 900],  # 0, 36\n",
    "#     \"afternoon\": [901, 1500],  # 37, 108\n",
    "#     \"late-afternoon\": [1501, 1800],  # 109, \n",
    "# }\n",
    "\n",
    "# def get_time_str(hrtime):\n",
    "#     int_hrtime = int(hrtime)\n",
    "#     for key in time_period:\n",
    "#         if (int_hrtime >= time_period[key][0]) and (int_hrtime <= time_period[key][1]):\n",
    "#             return key\n",
    "\n",
    "# def get_hrtime_by_filename(filename):\n",
    "#     fn1 = filename.replace(\".txt\", \"\").replace(\".npy\", \"\")\n",
    "#     datetime = fn1.split(\"_\")[-1]\n",
    "#     date, hrtime = datetime.split(\"t\")\n",
    "#     return hrtime\n",
    "\n",
    "gwmt_list = []\n",
    "\n",
    "mt_list = []\n",
    "root_list = []\n",
    "region_list = []\n",
    "value_list = []\n",
    "\n",
    "time_list = dict()\n",
    "period_list = dict()\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataset_path = os.path.join(\"data\", dataset)\n",
    "    for froot, di, files in os.walk(dataset_path):\n",
    "\n",
    "        def key(s):\n",
    "            try:\n",
    "                int(s)\n",
    "                return int(s)\n",
    "            except ValueError:\n",
    "                return len(files) + 1\n",
    "\n",
    "        def isSegmentation(s: str):\n",
    "            return \"segmentation\" in s\n",
    "\n",
    "        def endsWithTxt(s: str):\n",
    "            return s.endswith(\"txt\")\n",
    "\n",
    "        def endsWithNpy(s: str):\n",
    "            return s.endswith(\"npy\")\n",
    "\n",
    "        txt_files = list(filter(endsWithTxt, files))\n",
    "        txt_files.sort(key=lambda x: key(x.split(\".\")[0].split(\"_\")[-1]))\n",
    "\n",
    "        # You need to specify the root node type. Choices: [\"minimum\", \"maximum\"]\n",
    "        # (Avoid specifying merge tree type to avoid confusion between split tree and join tree in different contexts)\n",
    "        for file in txt_files:\n",
    "            trees, roots = rmt.get_trees(os.path.join(dataset_path, file), root_type=\"minimum\", threshold=value_thres)\n",
    "            mt_list.extend(trees)\n",
    "            root_list.extend(roots)\n",
    "\n",
    "        for file in txt_files:\n",
    "            regions, values = rmt.get_regions(os.path.join(dataset_path, file))\n",
    "            region_list.extend(regions)\n",
    "            value_list.extend(values)\n",
    "\n",
    "    assert (len(root_list) == len(mt_list))\n",
    "    assert (len(region_list) == len(mt_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2ce6e9",
   "metadata": {},
   "source": [
    "## Apply area-based intra-cloud anchor point simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a12329d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initially removing 29699 leaves.\n",
      "59184 3656\n",
      "Initially removing 29371 leaves.\n",
      "58576 3629\n",
      "Initially removing 28604 leaves.\n",
      "57042 3598\n",
      "Initially removing 28060 leaves.\n",
      "55993 3640\n",
      "Initially removing 27419 leaves.\n",
      "54724 3620\n",
      "Initially removing 26814 leaves.\n",
      "53514 3588\n",
      "Initially removing 26235 leaves.\n",
      "52405 3638\n",
      "Initially removing 25765 leaves.\n",
      "51469 3584\n",
      "Initially removing 25368 leaves.\n",
      "50692 3614\n",
      "Initially removing 24939 leaves.\n",
      "49826 3590\n",
      "Initially removing 24502 leaves.\n",
      "48981 3568\n",
      "Initially removing 24021 leaves.\n",
      "48025 3520\n",
      "Initially removing 23759 leaves.\n",
      "47499 3517\n",
      "Initially removing 23533 leaves.\n",
      "47029 3398\n",
      "Initially removing 23239 leaves.\n",
      "46466 3372\n",
      "Initially removing 22844 leaves.\n",
      "45654 3299\n",
      "Initially removing 22508 leaves.\n",
      "45035 3249\n",
      "Initially removing 22244 leaves.\n",
      "44500 3240\n",
      "Initially removing 22177 leaves.\n",
      "44405 3252\n",
      "Initially removing 22328 leaves.\n",
      "44649 3234\n",
      "Initially removing 22101 leaves.\n",
      "44170 3161\n",
      "Initially removing 21994 leaves.\n",
      "43998 3064\n",
      "Initially removing 22117 leaves.\n",
      "44214 3048\n",
      "Initially removing 21944 leaves.\n",
      "43878 3086\n",
      "Initially removing 21846 leaves.\n",
      "43708 3109\n",
      "Initially removing 21861 leaves.\n",
      "43707 3110\n",
      "Initially removing 21936 leaves.\n",
      "43878 3068\n",
      "Initially removing 22141 leaves.\n",
      "44240 3044\n"
     ]
    }
   ],
   "source": [
    "# Let's not oversimplify the merge tree, because now we need many nodes as anchor points\n",
    "\n",
    "# This serves for removing the very-small cloud system from the results entirely\n",
    "# This should be very small\n",
    "disappear_volume_threshold = 10\n",
    "\n",
    "# This is to reduce the number of anchor points (but not remove) for cloud systems\n",
    "# This can be a bit large\n",
    "volume_threshold = 30\n",
    "\n",
    "simplified_mt_list = [None for i in range(len(mt_list))]\n",
    "simplified_root_list = [None for i in range(len(mt_list))]\n",
    "\n",
    "idx = 0\n",
    "\n",
    "for mt, region in zip(mt_list, region_list):\n",
    "    if volume_threshold > 0:\n",
    "        _, simp_mt = volume_simplify_mt(mt, \n",
    "                                        vol_thres=volume_threshold, \n",
    "                                        disappear_vol_thres=disappear_volume_threshold, \n",
    "                                        vol_name=\"volume\", \n",
    "                                        stop_saddle_val=value_thres)\n",
    "        simplified_mt_list[idx] = simp_mt\n",
    "        simplified_root_list[idx] = simp_mt.root\n",
    "    else:\n",
    "        simplified_mt_list[idx] = mt\n",
    "        simplified_root_list[idx] = mt.root\n",
    "    print(mt.number_of_nodes(), simplified_mt_list[idx].number_of_nodes())\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55030e1e",
   "metadata": {},
   "source": [
    "## We save all the remaining critical point information as a list\n",
    "\n",
    "- Key information: \"x\", \"y\", \"z\", \"CriticalType\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41f9f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_info_root = \"./simplified-merge-trees/\"\n",
    "\n",
    "for dataset in datasets:\n",
    "    cp_info_dir = os.path.join(cp_info_root, dataset)\n",
    "    os.makedirs(cp_info_dir, exist_ok=True)\n",
    "\n",
    "    for em, mt in enumerate(simplified_mt_list):\n",
    "        crit_pts = []\n",
    "        for node in mt.nodes():\n",
    "            crit_pts.append([mt.nodes[node][\"x\"], mt.nodes[node][\"y\"], mt.nodes[node][\"z\"], mt.nodes[node][\"height\"], mt.nodes[node][\"type\"]])\n",
    "        cp_file = os.path.join(cp_info_dir, \"treeNodes_{}.txt\".format(str(em).zfill(3)))\n",
    "        with open(cp_file, \"w\") as outf:\n",
    "            for x, y, z, height, tp in crit_pts:\n",
    "                print(int(x), int(y), int(z), height, int(tp), file=outf)\n",
    "    #     crit_pts = np.asarray(crit_pts, dtype=int)\n",
    "    #     np.savetxt(cp_file, crit_pts, fmt=\"%d\")\n",
    "\n",
    "        edges = []\n",
    "        for node in mt.nodes():\n",
    "            for neighbor in mt.neighbors(node):\n",
    "                assert node < mt.number_of_nodes()\n",
    "                if neighbor > node:\n",
    "                    edges.append([node, neighbor])\n",
    "        edge_file = os.path.join(cp_info_dir, \"treeEdges_{}.txt\".format(str(em).zfill(3)))\n",
    "        edges = np.asarray(edges, dtype=int)\n",
    "\n",
    "        assert len(edges) == len(crit_pts) - 1\n",
    "        np.savetxt(edge_file, edges, fmt=\"%d\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adf2582",
   "metadata": {},
   "source": [
    "## We put all anchor points at a time step into a list for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00ecb00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchor_points_list = []\n",
    "\n",
    "# for mt in simplified_mt_list:\n",
    "#     anchor_pts = []\n",
    "#     for node in mt.nodes():\n",
    "#         if mt.nodes[node][\"type\"] == 2:\n",
    "#             anchor_pts.append({\"id\":node, \"x\":mt.nodes[node][\"x\"], \"y\":mt.nodes[node][\"y\"]})\n",
    "#     anchor_points_list.append(anchor_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d871f177",
   "metadata": {},
   "source": [
    "# Watershed Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e7a4d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.segmentation import watershed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3d3090",
   "metadata": {},
   "source": [
    "First, we visualize all the cloud areas above the superlevel set threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e62089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # value_thres = 2.0\n",
    "# time_i = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01109c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalar_field = value_list[time_i]\n",
    "# anchor_points = anchor_points_list[time_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c767f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_map = np.zeros(scalar_field.shape)\n",
    "# binary_map[scalar_field >= value_thres] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "806ce24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(binary_map, interpolation=\"none\")\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f1afd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Watershed tests\n",
    "\n",
    "# # markers are the coordinates from anchor points, but must be INT\n",
    "# markers = np.asarray([[int(each[\"x\"]), int(each[\"y\"])] for each in anchor_points], dtype=int)\n",
    "# print(markers.shape, np.max(markers[:, 0]), np.max(markers[:, 1]))\n",
    "# print(scalar_field.shape)\n",
    "                    \n",
    "# plt.imshow(binary_map, interpolation=\"none\")\n",
    "# plt.scatter(x=markers[:, 1], \n",
    "#             y=markers[:, 0],\n",
    "#             s=2)\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb9a5c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.ndimage import label\n",
    "# markers_in_field = np.zeros(scalar_field.shape, dtype=bool)\n",
    "# markers_in_field[tuple(markers.T)] = True\n",
    "# markers_with_label, _ = label(markers_in_field, structure=np.asarray([[0, 0, 0],[0, 1, 0],[0, 0, 0]]))\n",
    "\n",
    "# labels = watershed(-scalar_field, markers=markers_with_label, connectivity=2, mask=binary_map)\n",
    "\n",
    "# plt.figure(figsize=(16, 10))\n",
    "# plt.imshow(labels, interpolation=\"none\")\n",
    "# plt.colorbar()\n",
    "# plt.scatter(x=markers[:, 1], \n",
    "#             y=markers[:, 0],\n",
    "#             s=2,\n",
    "#             c=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15d3d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16, 10))\n",
    "# plt.imshow(markers_with_label, interpolation=\"none\")\n",
    "# plt.colorbar()\n",
    "# # plt.scatter(x=markers[:, 1], \n",
    "# #             y=markers[:, 0],\n",
    "# #             s=2,\n",
    "# #             c=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf734c8",
   "metadata": {},
   "source": [
    "We briefly report some statistics to this segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab744445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_index, label_counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "# print(len(markers))\n",
    "# print(np.sum(markers_in_field))\n",
    "# print(np.max(markers_with_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f5c4eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlabel_index, mlabel_counts = np.unique(markers_with_label, return_counts=True)\n",
    "# print(len(mlabel_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3c28b1",
   "metadata": {},
   "source": [
    "### The histogram for the segmentation area. \n",
    "\n",
    "A majority of cloud segmentations are small in area, but there are also regions with >6000 pixels in area. \n",
    "It casts doubt on whether using the area as the probability is stable, because the range of the probability of nodes can be very large. (e.g., 6000:10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce9b785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(label_counts[1:], bins=200)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c547e4",
   "metadata": {},
   "source": [
    "### Unit-based probability distribution\n",
    "\n",
    "We distribute a total probability of 1 to all anchor points based on the area of the segmentation. \n",
    "Therefore, the probability assigned to a node is (seg_area) * 1/(total_cloud_area), \n",
    "\n",
    "The 1/(total_cloud_area) is the unit for the probability in this case.\n",
    "\n",
    "Notes for total cloud area: \n",
    "* time_0: 281231\n",
    "* time_1: 250313\n",
    "* time_2: 245292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "700bba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Total cloud area:\", np.sum(label_counts[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e7907c",
   "metadata": {},
   "source": [
    "## Pairwise Region distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcfcc349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_j = time_i + 1\n",
    "# print(time_i, time_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "223b460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_segmentation(t):\n",
    "#     scalar_field = value_list[t]\n",
    "#     anchor_points = anchor_points_list[t]\n",
    "    \n",
    "#     binary_map = np.zeros(scalar_field.shape)\n",
    "#     binary_map[scalar_field >= value_thres] = 1\n",
    "    \n",
    "#     markers = np.asarray([[int(each[\"x\"]), int(each[\"y\"])] for each in anchor_points], dtype=int)\n",
    "#     markers_in_field = np.zeros(scalar_field.shape, dtype=bool)\n",
    "#     markers_in_field[tuple(markers.T)] = True\n",
    "#     markers_with_label, _ = label(markers_in_field, structure=np.asarray([[0, 0, 0],[0, 1, 0],[0, 0, 0]]))\n",
    "\n",
    "#     labels = watershed(-scalar_field, markers=markers_with_label, connectivity=2, mask=binary_map)\n",
    "#     return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4935520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_i = get_segmentation(time_i)\n",
    "# seg_j = get_segmentation(time_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b9f3a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist_ij = segmentation_distance(seg_i, seg_j, max_dist=18, normalize_factor=932, max_workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a82df1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(dist_ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f0b60cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Let's try some example filtering\n",
    "# feature_i = np.random.randint(len(label_counts[1:])) + 1 # np.argmax(label_counts[1:]) + 1\n",
    "# feature_i_map = np.zeros(seg_i.shape)\n",
    "# feature_i_map[seg_i == feature_i] = 1\n",
    "\n",
    "# feature_j_map = np.zeros(seg_j.shape)\n",
    "\n",
    "# idx_i = feature_i - 1\n",
    "# for idx_j in range(dist_ij.shape[1]):\n",
    "#     feature_j = idx_j + 1\n",
    "#     if dist_ij[idx_i, idx_j] < 932:\n",
    "#         feature_j_map[seg_j == feature_j] = 10 + dist_ij[idx_i, idx_j]\n",
    "\n",
    "# plt.figure(figsize=(18, 12))\n",
    "# plt.subplot(211)\n",
    "# plt.imshow(feature_i_map)\n",
    "\n",
    "# plt.subplot(212)\n",
    "# plt.imshow(feature_j_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499f5968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec323efc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
