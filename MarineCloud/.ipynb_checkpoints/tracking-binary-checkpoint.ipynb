{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b96a65",
   "metadata": {},
   "source": [
    "# pFGW Framework for Feature Matching & Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8a2a508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import networkx as nx\n",
    "import gc\n",
    "import time\n",
    "\n",
    "sys.path.append(\"../GWMT/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d085d287-497a-4093-9854-f894553cafb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GWMT import *\n",
    "import readMergeTree as rmt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8460f2",
   "metadata": {},
   "source": [
    "## Data Input "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d510d81b",
   "metadata": {},
   "source": [
    "This is an example to read our preset datasets. Our merge tree data is split into two files: ``treeEdges_monoMesh_*.txt`` and ``treeNodes_monoMesh_*.txt``, representing the data for tree edges and tree nodes, respectively.\n",
    "\n",
    "To read your own dataset, your tree data should be saved in a ``nx.Graph`` object, in which each node has properties for its spacial coordinates (e.g., \"x\", \"y\"), its scalar value (e.g., \"height\"), and its critical type (0: minimum, 1: saddle, 2: maximum. This setting cannot be changed). \n",
    "Besides, you also need to provide the node id for the root node of the tree.\n",
    "\n",
    "The tree data should be stored in a ``GWMergeTree`` object.\n",
    "\n",
    "==========================================================\n",
    "\n",
    "Our tree edge data format:\n",
    "\n",
    "a_0, b_0  \n",
    "a_1, b_1  \n",
    "...  \n",
    "a_{|E|-1}, b_{|E|-1}\n",
    "\n",
    "Each row describes the indices of two nodes that the edge connecting in between. Edges are undirected.\n",
    "\n",
    "==========================================================\n",
    "\n",
    "Our tree node data format:\n",
    "\n",
    "x_0, y_0, z_0, scalar_0, type_0  \n",
    "x_1, y_1, z_1, scalar_1, type_1  \n",
    "...  \n",
    "x_{|V|-1}, y_{|V|-1}, z_{|V|-1}, scalar_{|V|-1}, type_{|V|-1}\n",
    "\n",
    "Each row has five components: the \"x\", \"y\", \"z\" coordinates, the scalar value, and the critical point type for the node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97ef33cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "            \"CPPin20230801_0percent\",\n",
    "            # \"CPPin20230802_0percent\", \n",
    "            # \"CPPin20230803_0percent\",\n",
    "            # \"CPPin20230804_0percent\", \n",
    "            # \"CPPin20230805_0percent\", \n",
    "            # \"CPPin20230806_0percent\",\n",
    "            # \"CPPin20230807_0percent\", \n",
    "            # \"CPPin20230808_0percent\",\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68dc843-bb78-4e52-bb0c-0b9d765214d2",
   "metadata": {},
   "source": [
    "We merged several processing steps in the following code block.\n",
    "\n",
    "This is because we need to remove intermediate variables using garbage collection (gc).\n",
    "If the variables exist in multiple blocks, it is hard to release their memory even after we delete them in Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e54b0f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on CPPin20230801_0percent\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Adding volume to the merge tree\n",
      "Working on CPPin20230801_0percent\n",
      "Initially removing 30804 leaves.\n",
      "Initially removing 30670 leaves.\n",
      "Initially removing 30290 leaves.\n",
      "Initially removing 29574 leaves.\n",
      "Initially removing 29025 leaves.\n",
      "Initially removing 28592 leaves.\n",
      "Initially removing 28252 leaves.\n",
      "Initially removing 27510 leaves.\n",
      "Initially removing 27332 leaves.\n",
      "Initially removing 27027 leaves.\n",
      "Initially removing 26806 leaves.\n",
      "Initially removing 26655 leaves.\n",
      "Initially removing 26340 leaves.\n",
      "Initially removing 26132 leaves.\n",
      "Initially removing 25935 leaves.\n",
      "Initially removing 25594 leaves.\n",
      "Initially removing 25560 leaves.\n",
      "Initially removing 25277 leaves.\n",
      "Initially removing 25446 leaves.\n",
      "Initially removing 25578 leaves.\n",
      "Initially removing 25592 leaves.\n",
      "Initially removing 25772 leaves.\n",
      "Initially removing 26073 leaves.\n",
      "Initially removing 25699 leaves.\n",
      "Initially removing 25192 leaves.\n",
      "Initially removing 25239 leaves.\n",
      "Initially removing 25593 leaves.\n",
      "Initially removing 25886 leaves.\n",
      "Benchmark information - Simplify Merge Tree: 61359 2308 28 89.37018830003217\n",
      "Working on CPPin20230801_0percent\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1494234"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxima_only = True\n",
    "value_thres = 2.0\n",
    "\n",
    "mt_list = dict()\n",
    "root_list = dict()\n",
    "\n",
    "# Let's not oversimplify the merge tree, because now we need many nodes as anchor points\n",
    "\n",
    "# This serves for removing the very-small cloud system from the results entirely\n",
    "# This should be very small\n",
    "disappear_volume_threshold = 10\n",
    "\n",
    "# This is to reduce the number of anchor points (but not remove) for cloud systems\n",
    "# This can be a bit large\n",
    "volume_threshold = 30\n",
    "\n",
    "simplified_mt_list = dict()\n",
    "simplified_root_list = dict()\n",
    "\n",
    "# Loading the dataset (Merge tree)\n",
    "for dataset in datasets:\n",
    "    print(\"Working on\", dataset)\n",
    "    dataset_path = os.path.join(\"../data\", dataset)\n",
    "    mt_list[dataset] = []\n",
    "    root_list[dataset] = []\n",
    "    \n",
    "    for froot, di, files in os.walk(dataset_path):\n",
    "\n",
    "        def key(s):\n",
    "            try:\n",
    "                int(s)\n",
    "                return int(s)\n",
    "            except ValueError:\n",
    "                return len(files) + 1\n",
    "\n",
    "        def endsWithTxt(s: str):\n",
    "            return s.endswith(\"txt\")\n",
    "\n",
    "        def endsWithNpy(s: str):\n",
    "            return s.endswith(\"npy\")\n",
    "\n",
    "        txt_files = list(filter(endsWithTxt, files))\n",
    "        txt_files.sort(key=lambda x: key(x.split(\".\")[0].split(\"_\")[-1]))\n",
    "\n",
    "        # You need to specify the root node type. Choices: [\"minimum\", \"maximum\"]\n",
    "        # (Avoid specifying merge tree type to avoid confusion between split tree and join tree in different contexts)\n",
    "        for file in txt_files:\n",
    "            trees, roots = rmt.get_trees(os.path.join(dataset_path, file), root_type=\"minimum\", threshold=value_thres)\n",
    "            mt_list[dataset].extend(trees)\n",
    "            root_list[dataset].extend(roots)\n",
    "\n",
    "    assert (len(root_list[dataset]) == len(mt_list[dataset]))\n",
    "\n",
    "# For benchmark usage only\n",
    "max_raw_nodes = 0\n",
    "max_simplified_nodes = 0\n",
    "total_raw_nodes = 0\n",
    "total_simplified_nodes = 0\n",
    "n_runs = 0\n",
    "mt_simplification_runtime = 0\n",
    "\n",
    "# We simplify merge trees with volume thresholds\n",
    "for dataset in datasets:\n",
    "    print(\"Working on\", dataset)\n",
    "    simplified_mt_list[dataset] = [None for i in range(len(mt_list[dataset]))]\n",
    "    simplified_root_list[dataset] = [None for i in range(len(mt_list[dataset]))]\n",
    "\n",
    "    idx = 0\n",
    "\n",
    "    for mt in mt_list[dataset]:\n",
    "        if volume_threshold > 0:\n",
    "            # For benchmark purpose\n",
    "            simplify_t0 = time.perf_counter()\n",
    "            \n",
    "            _, simp_mt = volume_simplify_mt(mt, \n",
    "                                            vol_thres=volume_threshold, \n",
    "                                            disappear_vol_thres=disappear_volume_threshold, \n",
    "                                            vol_name=\"volume\", \n",
    "                                            stop_saddle_val=value_thres)\n",
    "            \n",
    "            # For benchmark purpose\n",
    "            simplify_dt = time.perf_counter() - simplify_t0\n",
    "            n_runs += 1\n",
    "            mt_simplification_runtime += simplify_dt\n",
    "            \n",
    "            simplified_mt_list[dataset][idx] = simp_mt\n",
    "            simplified_root_list[dataset][idx] = simp_mt.root\n",
    "            \n",
    "            max_raw_nodes = max(max_raw_nodes, mt.number_of_nodes())\n",
    "            max_simplified_nodes = max(max_simplified_nodes, simp_mt.number_of_nodes())\n",
    "            total_raw_nodes += mt.number_of_nodes()\n",
    "            total_simplified_nodes += simp_mt.number_of_nodes()\n",
    "            \n",
    "        else:\n",
    "            simplified_mt_list[dataset][idx] = mt\n",
    "            simplified_root_list[dataset][idx] = mt.root\n",
    "            \n",
    "        idx += 1\n",
    "\n",
    "print(\"Benchmark information - Simplify Merge Tree:\", max_raw_nodes, max_simplified_nodes, n_runs, mt_simplification_runtime)\n",
    "        \n",
    "for dataset in datasets:\n",
    "    lmt = len(mt_list[dataset])\n",
    "    mt_list[dataset] = [None] * lmt\n",
    "\n",
    "# We put the simplified merge trees into the GWMergeTree object\n",
    "gwmt_list = dict()\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Working on\", dataset)\n",
    "    gwmt_list[dataset] = [GWMergeTree(simplified_mt_list[dataset][i], simplified_root_list[dataset][i]) for i in range(len(mt_list[dataset]))]\n",
    "\n",
    "for dataset in datasets:\n",
    "    lmt = len(simplified_mt_list[dataset])\n",
    "    simplified_mt_list[dataset] = [None] * lmt\n",
    "    simplified_root_list[dataset] = [None] * lmt\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba3c5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark information - Simplify Merge Tree (Total #nodes): 1502783 57975\n"
     ]
    }
   ],
   "source": [
    "print(\"Benchmark information - Simplify Merge Tree (Total #nodes):\", total_raw_nodes, total_simplified_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ccc809",
   "metadata": {},
   "source": [
    "## Parameter Initialization\n",
    "\n",
    "We now specify the parameters to be passed to the pFGW framework, including the following:\n",
    "\n",
    "*spacial_scale*: a float number denoting the spacial length of the scalar field. (e.g., $max(height, width)$ for a 2D rectangular scalar field).  \n",
    "         This is needed for normalizing the spacial coordinates for nodes.  \n",
    "         This number can be flexible. However, it affects the optimal $\\alpha$ value. Please make sure that the normalization is reasonable. A recommended range for the maximum Euclidean distance between any pair of critical points after normalization is $[0.5, 2.0]$.\n",
    "         \n",
    "\n",
    "*labels*: a list of strings denoting the spacial coordinate labels. (e.g., [\"x\", \"y\"] for a 2D scalar field)\n",
    "\n",
    "*scalar_name*: the name of the scalar field in GWMergeTree objects.\n",
    "\n",
    "*edge_weight_name*: the name of the weight of edges in GWMergeTree objects.\n",
    "\n",
    "*weight_mode*: the strategy to encode $W$. Choices: [\"shortestpath\", \"lca\"].\n",
    "\n",
    "*prob_distribution*: the strategy to encode $p$. Choices: [\"uniform\", \"ancestor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d9819ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"CloudDensity\" in dataset:\n",
    "    spacial_scale = 255 \n",
    "elif \"CPPin202308\" in dataset:\n",
    "    spacial_scale = 932\n",
    "elif \"juelich\" in dataset:\n",
    "    spacial_scale = 427\n",
    "    \n",
    "max_value = 150\n",
    "labels = [\"x\", \"y\", \"z\"]\n",
    "scalar_name = \"height\"\n",
    "edge_weight_name = \"weight\"\n",
    "weight_mode = \"shortestpath\"\n",
    "prob_distribution = \"uniform\"\n",
    "fully_initialized = False\n",
    "\n",
    "# lca-threshold\n",
    "intrinsic_rescale = 1\n",
    "if weight_mode == \"lca\":\n",
    "    intrinsic_rescale = max_value\n",
    "elif weight_mode == \"shortestpath\":\n",
    "    intrinsic_rescale = max_value * 2\n",
    "    \n",
    "all_scales = [spacial_scale, spacial_scale, spacial_scale, intrinsic_rescale]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "305bea04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on CPPin20230801_0percent\n"
     ]
    }
   ],
   "source": [
    "# Validify the GWMergeTree object. This is not mandatory, but recommended to check whether your data input format is correct\n",
    "for dataset in datasets:\n",
    "    print(\"Working on\", dataset)\n",
    "    for tree in gwmt_list[dataset]:\n",
    "        tree.label_validation(labels, scalar_name, edge_weight_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37304c4",
   "metadata": {},
   "source": [
    "## Framework Initialization\n",
    "\n",
    "Initializing the pFGW framework with given parameters and data input. There is an output for the normalization process consisting $L+1$ elements, where the first $L$ elements are $\\frac{1}{rescaling\\_factor}$ for coordinate labels ($L$ coordinate labels in total), and the last element is $\\frac{1}{rescaling\\_factor}$ for scalar function values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80fe6301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on CPPin20230801_0percent\n",
      "WARNING! We do not compute the scale for all time steps.\n",
      "Please provide the scale for normalization when intialize steps.\n"
     ]
    }
   ],
   "source": [
    "pfgws = dict()\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Working on\", dataset)\n",
    "    pfgws[dataset] = GWTracking(\n",
    "        gwmt_list[dataset],\n",
    "        spacial_scale,\n",
    "        labels,\n",
    "        scalar_name=scalar_name,\n",
    "        edge_weight_name=edge_weight_name,\n",
    "        weight_mode=weight_mode,\n",
    "        prob_distribution=prob_distribution,\n",
    "        tracking_maxima_only=maxima_only,\n",
    "        maxima_only=maxima_only,\n",
    "        fully_initialized=fully_initialized,\n",
    "        scalar_threshold=value_thres\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "250f8177",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    pfgws[dataset].set_all_scales(all_scales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e47c09a",
   "metadata": {},
   "source": [
    "## Parameter Tuning\n",
    "\n",
    "We present a demo for our parameter tuning process in this section.\n",
    "\n",
    "This is not the only way for parameter tuning. We aim to optimize the one-to-one matching result in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16a21b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# The optional parameter \"amijo\" determines the way for OT solver to converge, which may change the result\n",
    "# Either picking False or True follows our framework pipeline. They just may reach different local optimal solutions.\n",
    "\n",
    "amijo = False\n",
    "max_dist_tuning = False\n",
    "\n",
    "# The max_workers refers to the max number of threads you want the parallel run to have\n",
    "max_dist = 0.0065\n",
    "max_workers = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab6275e",
   "metadata": {},
   "source": [
    "### Actual tracking with tuning the m parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c232b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_runs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfaa4d26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on CPPin20230801_0percent\n",
      "alpha = 0.4\n",
      "working from timestep 0 to 1...\n",
      "Finished intializing timestep 0 , CList shape: (1013,)\n",
      "Finished intializing timestep 1 , CList shape: (1108,)\n",
      "The optimal m for timestep 0 vs. 1 is 0.515.\n",
      "Max Matched Dist list: [np.float64(0.00625638615326754)]\n",
      "INIT runtime: [23.079040199983865, 29.82843499997398]\n",
      "pFGW runtime: [65.27913959999569]\n",
      "Initialization average runtime: 26.453737599978922\n",
      "pFGW average runtime: 65.27913959999569\n"
     ]
    }
   ],
   "source": [
    "# New!!! We are using auto parameter tuning strategies\n",
    "# For cloud tracking project, there is a forced search range for other cloud tracking tools\n",
    "# Therefore, we can safely assume the max matched distance without tuning\n",
    "alpha_list = [0.4]\n",
    "\n",
    "# m_range largely affects the runtime performance!!\n",
    "# The default eps of m search space is 1e-2. \n",
    "# Therefore, you need ceil(log_2((m_range[1]-m_range[0])*100)) computations to get the best m value.\n",
    "# If you can't make the difference in m_range below 0.16, then you should make it around 0.30 (e.g., [0.60, 0.90]), \n",
    "# in which case you need 5 runs to get the optimal m value.\n",
    "m_range = [0.60, 0.90]\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Working on\", dataset)\n",
    "    timesteps = list(range(len(gwmt_list[dataset])))\n",
    "\n",
    "    parameter_tuning_path = os.path.join(\"binary-parameter-tuning\", \"{}\".format(dataset))\n",
    "    os.makedirs(parameter_tuning_path, exist_ok=True)\n",
    "    oc_path = os.path.join(parameter_tuning_path, \"ocs\")\n",
    "    os.makedirs(oc_path, exist_ok=True)\n",
    "\n",
    "    for alpha in alpha_list:\n",
    "        if (dataset in finished_runs) and (alpha in finished_runs[dataset]):\n",
    "            continue\n",
    "        print(\"alpha =\", alpha)\n",
    "        best_ms, best_ocs, m_searchspace, dist_values, init_rt, pfgw_rt = pfgws[dataset].adaptive_m_tuning_binary_search(timesteps,\n",
    "                                                                                             alpha, \n",
    "                                                                                             max_dist, \n",
    "                                                                                             amijo, \n",
    "                                                                                             m_range, \n",
    "                                                                                             max_workers, \n",
    "                                                                                             metric=\"l2\", \n",
    "                                                                                             prob_rubric=\"nonzero\",\n",
    "                                                                                             benchmark=True)\n",
    "        \n",
    "        print(\"Initialization average runtime:\", init_rt)\n",
    "        print(\"pFGW average runtime:\", pfgw_rt)\n",
    "\n",
    "        save_binary_parameter_tuning(parameter_tuning_path, oc_path, alpha, best_ms, best_ocs, m_searchspace, dist_values)\n",
    "        if dataset not in finished_runs:\n",
    "            finished_runs[dataset] = [alpha]\n",
    "        else:\n",
    "            finished_runs[dataset].append(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a09175a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on CPPin20230801_0percent\n",
      "alpha = 0.4\n",
      "0.515\n"
     ]
    }
   ],
   "source": [
    "# We save the best-tuned results into the results folder\n",
    "for dataset in datasets:\n",
    "    print(\"Working on\", dataset)\n",
    "    if dataset not in finished_runs:\n",
    "        continue\n",
    "        \n",
    "    parameter_tuning_path = os.path.join(\"binary-parameter-tuning\", \"{}\".format(dataset))\n",
    "    oc_path = os.path.join(parameter_tuning_path, \"ocs\")\n",
    "    timesteps = list(range(len(gwmt_list[dataset])))\n",
    "\n",
    "    output_root = os.path.join(\"./initial-output/\", dataset)\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "    for e, alpha in enumerate(alpha_list):\n",
    "        if alpha not in finished_runs[dataset]:\n",
    "            continue\n",
    "            \n",
    "        print(\"alpha = {}\".format(str(alpha)))\n",
    "        best_ms, best_ocs, _, _ = load_binary_parameter_tuning(parameter_tuning_path, oc_path, alpha)\n",
    "\n",
    "        output_path = os.path.join(output_root, str(round(alpha, 1)))\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        for em, m in enumerate(best_ms):\n",
    "            print(str('%.3f' % m))\n",
    "            # We filter out coupling information unrelated to maxima\n",
    "            # Note: the filtered coupling matrix does not sum to m\n",
    "            id1 = timesteps[em]\n",
    "            id2 = timesteps[em+1]\n",
    "\n",
    "            oc = best_ocs[em]\n",
    "            oc_maxima = oc # pfgw.filtered_oc(id1, id2, oc)\n",
    "            np.savetxt(os.path.join(output_path, \"oc_{}_{}.txt\".format(str(id1), str(id2))), oc_maxima)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2872a1e-ce01-41ef-8217-748aa310a180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
